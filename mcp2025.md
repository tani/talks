---
marp: true
theme: default
---

# MCPの紹介

Masaya Taniguchi

---

## MCPってなに

> MCPはLLMsに文脈をどう渡すかを標準化した開かれたプロトコルです。
> https://modelcontextprotocol.io/


---

## MCPサーバーの普及状況

有名サービス・ソフトウェアが続々と公式MCPサーバーを公開
- [MCP Server for Windows](https://blogs.windows.com/windowsexperience/2025/05/19/securing-the-model-context-protocol-building-a-safer-agentic-future-on-windows/)
- [GitHub's official MCP Server ](https://github.com/github/github-mcp-server)
- [Notion MCP Server](https://github.com/makenotion/notion-mcp-server)
- [AWS MCP Servers](https://github.com/awslabs/mcp)

→ MCPサーバーをつくるだけで、LLMsがエージェント的に使ってくれる!

---

## LLMプロバイダの対応状況

- OpenAIはSDK経由で対応
  https://openai.github.io/openai-agents-python/ja/mcp/
- Anthropic は SDK/ Desktopアプリで対応
- Google は発表なし

## クライアントの対応状況

- [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
- [GitHub Copilot](https://docs.github.com/en/copilot/customizing-copilot/extending-copilot-chat-with-mcp)

---

## Tips: LLMs.txt

言語モデルにウェブサービスの内容をすべて読ませるための
プロトコルが Answer.ai (ModernBERTを開発したところ) いによって提案されている。ウェブサービスの内容をすべて txtファイルに書き出したものをサービス直下に置きましょうという提案。

mcp の公式ページにも llms.txt がある。これを NotebookLMに渡すと便利。

https://modelcontextprotocol.io/llms.txt
https://modelcontextprotocol.io/llms-full.txt

---

## MCP クライアントの使い方

1. `brew install --cask claude-desktop`

---

## MCP サーバーの登録

3. `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "git": {
      "command": "uvx",
      "args": [
        "mcp-server-time",
        "--local-timezone",
        "Asia/Tokyo"
      ]
    }
  }
}
```
---

## 実際に MCPサーバーを呼び出してみる。

---

## MCPプロトコルの概要

MCPサーバーは、主に以下の3種類の機能を提供できます。

- *プロンプト (Prompts)*: AIモデルとの対話をガイドするための事前に定義されたテンプレートや指示を提供します。これにより、ユーザーはより簡単にAIモデルと効果的にコミュニケーションをとることができます。
- *リソース (Resources)*: AIモデルに追加のコンテキスト（文脈情報）を提供するための構造化されたデータやコンテンツへのアクセスを可能にします。例えば、ファイルシステム、データベース、API連携などがこれに該当します。
- *ツール (Tools)*: AIモデルが特定の操作を実行したり、情報を取得したりするための実行可能な機能を提供します。これにより、AIモデルは単に情報を処理するだけでなく、具体的なタスクを実行できるようになります。

<p align="center">
    <strong>使い方を知る → ターゲットを読み込む → ターゲットへ書き込む</strong>
</p>

---

## Python でつくる MCP サーバー

- ライブラリはここに公開されている。
 https://github.com/modelcontextprotocol/python-sdk
- PyPIには `mcp` という名前で登録されてます: `uv add uv[cli]`

```python
# server.py
from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP("Demo")
```

---

## リソースを読み込む
- *リソース (Resources)*: AIモデルに追加のコンテキスト（文脈情報）を提供するための構造化されたデータやコンテンツへのアクセスを可能にします。例えば、ファイルシステム、データベース、API連携などがこれに該当します。
- Python のデコレータを使って、関数をリソースとして登録する。

```python
# Add a dynamic greeting resource
@mcp.resource("greeting://{name}")
def get_greeting(name: str) -> str:
    """Get a personalized greeting"""
    return f"Hello, {name}!"
```

---

## ツールを使う

- Python のデコレータを使って、関数をリソースとして登録する。
- *ツール (Tools)*: AIモデルが特定の操作を実行したり、情報を取得したりするための実行可能な機能を提供します。これにより、AIモデルは単に情報を処理するだけでなく、具体的なタスクを実行できるようになります。
```python
# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b
```

---

## MCPサーバーをインストールする

```shell
mcp install server.py
```

---

## MCPサーバーをデバッグする

```shell
mcp dev server.py
```

---

## 応用例: 数学の定理証明

- リソース: 数学の定理検索 (Mathlib4 から関連定理を検索)
- ツール：　数学の証明検証 (Lean4 のコンパイルエラーチェック)
- → 自動で調べて証明するエージェントの完成!

---

### 応用例: LLMを呼び出すLLM

- リソース: 事前に使える LLM一覧を取得
- ツール: LLMへの問合せ
- → 対話プロトコルの完成！